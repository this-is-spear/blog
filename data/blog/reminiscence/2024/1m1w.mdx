---
title: 1월 1주 있었던 일 정리
date: '2024-01-07'
tags: ['회고', '집킨', '키바나', '엘라스틱서치']
draft: false
summary: 우리 팀에 제일 필요한 것은?
images: []
---

## 분산 환경 추적 시스템 구축하기

### 요약

`12m4w`에서 이야기 했던 문제에 가설을 세우고 해결을 진행함.
요약하자면 `서비스와 함께 성장하는 모니터링 시스템이 필요해.`임.
필요한 건 다음과 같음.

- 시스템 문제는 `비용 파악이 어려운 상황`, `문제 추적이 어려운 상황`, `지연이 발생하는 시점을 파악하기 어려운 상황` 이 존재함.
- 서비스 로그는 `사용자들의 동향`을 파악할 수 있고 `재진입성을 높일 수 있는 방법`으로 보임.

상황에 맞게 서비스와 함께 성장할 로그 관리 방법이 필요했음.

### 요구사항

기획 단계에서 얻고자 하는 지표가 자주 변경되는 이슈가 있었음. 로그 변경마다 서버 재시작하는 건 꽤나 까다로운 조건임.
그래서 `logstash` 를 활용해 필터링 쉽게 수정할 수 있도록 구성했음.

여러 프로젝트를 자세히 찾아보니 이전 개발자 분이 `logback-logstash` 연동이 쉬운 라이브러리를 이미 적용했음!!
그래서 `logback`에서 `logstash` 라이브러리 활용할 수 있도록 테스트 환경 구성중임.

가장 중요한건 이해관계자들의 적극성을 유도하는 일이었음.
쿼리를 날리는건 이해 관계자들에게 비싼 작업이었고, 그로 인해 데이터 기반 의사 결정의 진보가 더딜 수 있다 판단함.
그래서 `kibana` 에서 통계 대시보드로 쉽게 시각화 할 수 있도록 구성함.

분산 환경에서 추적하기 쉬운 툴로 `elasticsearch apm` 사용해보려 함.
`zipkin`을 사용하면 데이터 베이스 관리가 여간 힘든게 아님.
차라리 `zipkin`이 `db`와 붙어 있었다면 사용했겠지만, `db`와의 트러블 슈팅을 고려하긴 걱정이 많았음.
그래서 `db`, `apm`을 같이 제공하는 `elastic`을 활용하기로 함.

### 다음은,,,

성장하다보면 적은 사용자를 기준으로 설계한 시스템에 무리가 갈 수 있을거라고 판단해 적정 수준까지 개선해야 할 사항과 방법을 정리하기로 함.

- `fleet`과 여러 서버에서 실행되는 `apm` 구축
- `elasticsearch` 클러스터 구축
- `beats` 구성

### 중요하게 생각한 일들

중요하게 생각한 건 다음과 같음.
조금 더 자세하게 쓰고 싶었는데, 이번 주는 apm 덕분에 많은 시간을 할애해서 시간이 얼마 남지 않았음.

- 이해 관계자와 협업하기 : 이해 관계자들의 퍼포먼스를 높이기 위해 데이터 접근에 어려운 과정 개선
- 오버 엔지니어링 피하기 : 초기 비용을 줄이고 점진적인 개선이 가능하도록 비용이 들지 않는 곳만 개선 및 개선 과정 문서화
- 현재 업무에 집중하기 : 시스템 모니터링은 현재 업무가 아님. 맡은 업무를 다하고 개선 할 것.
- `elasticsearch`에 집중하기 : `elasticsearch` 기능이 엄청 많음. 최대한 이해하고 사용해보기.

### 적용하기 위한 트러블 슈팅

업무에 적용하기 전에 분산 환경에 바로 녹여 낼 수 있도록 유사하지만 심플한 환경 구성 후 테스트 진행함.

#### 첫 번째 문제 - 추적이 불가능해요

`trace id`, `span id`, `transaction id`가 모두 할당되지만 추적이 안됨.
확인해보니 연결된 작업에도 trace id 가 달라서 시도가 불가능 했음.
`elastic apm`은 `traceparent` 헤더를 사용해서 추적하고 있었음.
그래서 `traceparent`를 추적해봤음.

> traceparent 는 version, trace id, parent id, trace-flags 가 포함된 정보임.
> 자세한 정보는 해당 [링크](https://www.w3.org/TR/trace-context/#traceparent-header)에서 확인 가능.

확인해보니 `traceparent`정보에서 `trace id`는 동일하지만 서버 내부에서 변형되어서 발생하는 문제였음.

`zipkin` 사용을 위해 `brave` 라이브러리를 사용했는데, 동일한 컨텍스트에서 저장되어 덮어써지는 것이 원인으로 보였음.
즉, `traceId`, `spanId` 같은 프로퍼티를 사용하기 때문에 충돌이 발생함.
제거 했더니 `traceparent` 정보와 일치하는 `trace id`, `span id`를 부여받았음.

이 문제의 원인을 쉽게 파악 할 수 없었는데, 해당 [문서](https://www.elastic.co/kr/blog/distributed-tracing-opentracing-and-elastic-apm)에서 `헤더가 다르니 충돌이 발생하지 않을거야!`라는 문장으로 `충돌은 없어!` 로 오해해서 그랬음.
전파되기 위해 서버에 저장되는 순간 충돌이 발생할 거란 상상하지도 못했음.

[분산 추적, OpenTracing 및 Elastic APM](https://www.elastic.co/kr/blog/distributed-tracing-opentracing-and-elastic-apm)

#### 두 번째 문제 - 어라 왜 service map 이 정상적으로 동작하지 않지

본인이 `elastic apm`을 선택했던 이유 중 하나가 트래픽을 쉽게 시각화 할 수 있는 장점 덕분이었음.

![images](/static/images/reminiscence/2024/1m1w/1.png)

그런데 정상적으로 동작하지 않음. 서비스 맵에서 게이트웨이가 추가되질 않는 거임.

![images](/static/images/reminiscence/2024/1m1w/2.png)

원인을 파악해보니 서비스와 게이트웨이의 부모만 같지 그외의 연관성은 없어 보임.
`parent`가 같은 경우는 동일한 컨텍스트에서 실행됨을 보여주기 위해서라고 생각함.
만약 게이트웨이와 서비스가 동일한 컨텍스트라고 생각하면 유지해도 되고 그렇지 않다면 분리가 필요해보임.

> elastic은 trace 컨텍스트에서 parent와 span으로 의존성을 유추하는 모습을 볼 수 있었음

![images](/static/images/reminiscence/2024/1m1w/3.png)

그래서 `transaction` 과 `trace`, `parent`의 컨텍스트가 어떻게 공유되는지 파악해봄.

![images](/static/images/reminiscence/2024/1m1w/4.png)

내 생각에는 동일한 `traceparent`가 변경되지 않고 전파되어서 발생하는 문제처럼 보임.
즉, 게이트웨이에서는 `traceparent` 변경이 발생하지 않음.

뭐 이런 [링크](https://discuss.elastic.co/t/not-support-spring-cloud-gateway/309595)도 확인해봤는데, 큰 진전은 없음.
코드 상으로 직접 새로운 `span`을 생성한 다음 전파하는 게 맞다고 생각함.

새로운 `span`을 생성하면 변경된 `parent`로 `traceparent`로 변경해주겠지? 하는게 나의 희망사항 임.
이것저것한다고 시간이 너무 지체됐고, 이후 작업이 불가능해 다음주에 진행하기로 함.


### 개발이란

이런 저런 문제들을 해결하면서 개발하려는 목표는 무엇일까 곰곰히 생각하게 됨.
개발 목표를 간단하게 두 가지로 분류했는데 재사용성을 높이거나, 세운 가설을 증명하거나 둘 중 하나로 보임.
결국 관리냐 새로 만드냐 차이 같음 ㅋㅋ

결국 현재 상황에서 더 나은 방향을 선택하는 일이라 생각함. 아래와 같은 패턴으로 계속 수행하지 않을까.

> 가설을 세운다. -> 가설을 증면한다. -> 사용자 행동을 유도한다. or 불편함을 개선한다. -> 가설을 세운다....

위 문제를 해결하면서 이런 프로세스로 전반적인 흐름이 이어지지 않나 생각 드는 한 주였음.
